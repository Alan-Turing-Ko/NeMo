{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import nemo\n",
    "from nemo.utils.lr_policies import WarmupAnnealing\n",
    "\n",
    "import nemo_nlp\n",
    "from nemo_nlp import NemoBertTokenizer\n",
    "from nemo_nlp.utils.callbacks.token_classification import \\\n",
    "    eval_iter_callback, eval_epochs_done_callback\n",
    "\n",
    "BATCHES_PER_STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "CLASSIFICATION_DROPOUT = 0.1\n",
    "\n",
    "# To download and preprocess the data run\n",
    "# python NeMo/scripts/get_tatoeba_data.py --data_dir DATA_DIR\n",
    "DATA_DIR = \"PATH TO WHERE THE DATA IS\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.00005\n",
    "LR_WARMUP_PROPORTION = 0.1\n",
    "OPTIMIZER = \"adam\"\n",
    "PRETRAINED_BERT_MODEL = \"bert-base-uncased\"\n",
    "\n",
    "# It's import to specify the none_label correctly depending on a task at hand.\n",
    "# For combined punctuation and capitalization task use 'OL' for a pucntuation only model the default 'O' will work\n",
    "NONE_LABEL = 'OL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate neural factory with supported backend\n",
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    backend=nemo.core.Backend.PyTorch,\n",
    "\n",
    "    # If you're training with multiple GPUs, you should handle this value with\n",
    "    # something like argparse. See examples/nlp/token_classification.py for an example.\n",
    "    local_rank=None,\n",
    "\n",
    "    # If you're training with mixed precision, this should be set to mxprO1 or mxprO2.\n",
    "    # See https://nvidia.github.io/apex/amp.html#opt-levels for more details.\n",
    "    optimization_level=\"O0\",\n",
    "\n",
    "    # If you're training with multiple GPUs, this should be set to\n",
    "    # nemo.core.DeviceType.AllGpu\n",
    "    placement=nemo.core.DeviceType.GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 16:45:09,962 - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2019-12-04 16:45:09,962 - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2019-12-04 16:45:09,962 - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2019-12-04 16:45:10,427 - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "2019-12-04 16:45:10,427 - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "2019-12-04 16:45:10,427 - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "2019-12-04 16:45:10,432 - INFO - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2019-12-04 16:45:10,432 - INFO - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2019-12-04 16:45:10,432 - INFO - Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2019-12-04 16:45:10,745 - INFO - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2019-12-04 16:45:10,745 - INFO - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2019-12-04 16:45:10,745 - INFO - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ebakhturina/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "# If you're using a standard BERT model, you should do it like this. To see the full\n",
    "# list of BERT model names, check out nemo_nlp.huggingface.BERT.list_pretrained_models()\n",
    "tokenizer = NemoBertTokenizer(pretrained_model=PRETRAINED_BERT_MODEL)\n",
    "bert_model = nemo_nlp.huggingface.BERT(pretrained_model_name=PRETRAINED_BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe training DAG\n",
    "train_data_layer = nemo_nlp.BertTokenClassificationDataLayer(tokenizer=tokenizer,\n",
    "                                                             text_file=os.path.join(DATA_DIR, 'text_train.txt'),\n",
    "                                                             label_file=os.path.join(DATA_DIR, 'labels_train.txt'),\n",
    "                                                             max_seq_length=MAX_SEQ_LENGTH,\n",
    "                                                             batch_size=BATCH_SIZE)\n",
    "\n",
    "label_ids = train_data_layer.dataset.label_ids\n",
    "num_classes = len(label_ids)\n",
    "\n",
    "hidden_size = bert_model.local_parameters[\"hidden_size\"]\n",
    "ner_classifier = nemo_nlp.TokenClassifier(hidden_size=hidden_size,\n",
    "                                          num_classes=num_classes,\n",
    "                                          dropout=0.1)\n",
    "\n",
    "ner_loss = nemo_nlp.TokenClassificationLoss(\n",
    "    d_model=bert_model.bert.config.hidden_size,\n",
    "    num_classes=len(tag_ids),\n",
    "    dropout=CLASSIFICATION_DROPOUT)\n",
    "\n",
    "input_ids, input_type_ids, input_mask, loss_mask, _, labels = train_data_layer()\n",
    "\n",
    "hidden_states = bert_model(\n",
    "    input_ids=input_ids,\n",
    "    token_type_ids=input_type_ids,\n",
    "    attention_mask=input_mask)\n",
    "\n",
    "logits = ner_classifier(hidden_states=hidden_states)\n",
    "loss = ner_loss(logits=logits, labels=labels, loss_mask=loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe evaluation DAG\n",
    "eval_data_layer = nemo_nlp.BertTokenClassificationDataLayer(\n",
    "        tokenizer=tokenizer,\n",
    "        text_file=os.path.join(DATA_DIR, 'text_dev.txt'),\n",
    "        label_file=os.path.join(DATA_DIR, 'labels_dev.txt'),\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "eval_input_ids, eval_input_type_ids, eval_input_mask, eval_loss_mask, eval_subtokens_mask, eval_labels \\\n",
    "    = eval_data_layer()\n",
    "\n",
    "hidden_states = bert_model(\n",
    "    input_ids=eval_input_ids,\n",
    "    token_type_ids=eval_input_type_ids,\n",
    "    attention_mask=eval_input_mask)\n",
    "\n",
    "eval_logits = ner_classifier(hidden_states=hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_train = nemo.core.SimpleLossLoggerCallback(\n",
    "    tensors=[loss],\n",
    "    print_func=lambda x: print(\"Loss: {:.3f}\".format(x[0].item())))\n",
    "\n",
    "train_data_size = len(train_data_layer)\n",
    "\n",
    "# If you're training on multiple GPUs, this should be\n",
    "# train_data_size / (batch_size * batches_per_step * num_gpus)\n",
    "steps_per_epoch = int(train_data_size / (BATCHES_PER_STEP * BATCH_SIZE))\n",
    "\n",
    "callback_eval = nemo.core.EvaluatorCallback(\n",
    "    eval_tensors=[eval_logits, eval_labels, eval_subtokens_mask],\n",
    "    user_iter_callback=lambda x, y: eval_iter_callback(x, y),\n",
    "    user_epochs_done_callback=lambda x: eval_epochs_done_callback(x, tag_ids),\n",
    "    eval_step=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_policy = WarmupAnnealing(NUM_EPOCHS * steps_per_epoch,\n",
    "                            warmup_ratio=LR_WARMUP_PROPORTION)\n",
    "optimizer = neural_factory.get_trainer()\n",
    "optimizer.train(\n",
    "    tensors_to_optimize=[loss],\n",
    "    callbacks=[callback_train, callback_eval],\n",
    "    lr_policy=lr_policy,\n",
    "    batches_per_step=BATCHES_PER_STEP,\n",
    "    optimizer=OPTIMIZER,\n",
    "    optimization_params={\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"lr\": LEARNING_RATE\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
